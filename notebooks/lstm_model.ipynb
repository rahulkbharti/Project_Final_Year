{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270b4fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ba8c5d6b-6904-4c67-b66b-ddaa904d77da",
       "rows": [
        [
         "0",
         "Hi, How are you doing?",
         "I am fine. How about yourself?"
        ],
        [
         "1",
         "I am fine. How about yourself?",
         "I am pretty good. Thanks for asking."
        ],
        [
         "2",
         "I am pretty good. Thanks for asking.",
         "No problem. So how have you been?"
        ],
        [
         "3",
         "No problem. So how have you been?",
         "I have been great. What about you?"
        ],
        [
         "4",
         "I have been great. What about you?",
         "I have been good. I am in school right now."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi, How are you doing?</td>\n",
       "      <td>I am fine. How about yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am fine. How about yourself?</td>\n",
       "      <td>I am pretty good. Thanks for asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am pretty good. Thanks for asking.</td>\n",
       "      <td>No problem. So how have you been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No problem. So how have you been?</td>\n",
       "      <td>I have been great. What about you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have been great. What about you?</td>\n",
       "      <td>I have been good. I am in school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0                Hi, How are you doing?   \n",
       "1        I am fine. How about yourself?   \n",
       "2  I am pretty good. Thanks for asking.   \n",
       "3     No problem. So how have you been?   \n",
       "4    I have been great. What about you?   \n",
       "\n",
       "                                        answer  \n",
       "0               I am fine. How about yourself?  \n",
       "1         I am pretty good. Thanks for asking.  \n",
       "2            No problem. So how have you been?  \n",
       "3           I have been great. What about you?  \n",
       "4  I have been good. I am in school right now.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the TSV dataset\n",
    "file_path = './dialogues_eda.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9965dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2980\n",
      "First training pair:\n",
      "Q: Because it has great teachers.\n",
      "A: What else?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is in a DataFrame called 'df'\n",
    "# Split into train and validation sets (keeping question-answer pairs intact)\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use ONLY the training data\n",
    "questions = train_df['question'].tolist()\n",
    "answers = train_df['answer'].tolist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Training samples: {len(questions)}\")\n",
    "print(f\"First training pair:\")\n",
    "print(f\"Q: {questions[0]}\")\n",
    "print(f\"A: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e3cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hi, how are you doing?\n",
      "Response: it sunday that going to pair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Special tokens\n",
    "STOP_TOKEN = '<end>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = defaultdict(lambda: len(self.word2idx))\n",
    "        self.idx2word = {}\n",
    "        # Add special tokens first\n",
    "        self.add_word(PAD_TOKEN)  # index 0\n",
    "        self.add_word(UNK_TOKEN)  # index 1\n",
    "        self.add_word(STOP_TOKEN)  # index 2\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            idx = self.word2idx[word]\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "def build_vocabulary(texts):\n",
    "    vocab = Vocabulary()\n",
    "    for text in texts:\n",
    "        for word in text.lower().split():\n",
    "            vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.word2idx.get(word, vocab.word2idx[UNK_TOKEN]) \n",
    "            for word in text.lower().split()]\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Question': questions,\n",
    "    'Answer': answers\n",
    "}\n",
    "\n",
    "# Build vocabulary\n",
    "all_texts = data['Question'] + data['Answer']\n",
    "vocab = build_vocabulary(all_texts)\n",
    "\n",
    "# Convert and pad sequences with stop token\n",
    "stop_token = vocab.word2idx[STOP_TOKEN]\n",
    "pad_token = vocab.word2idx[PAD_TOKEN]\n",
    "\n",
    "questions = [text_to_sequence(q, vocab) for q in data['Question']]\n",
    "answers = [text_to_sequence(a, vocab) + [stop_token] for a in data['Answer']]\n",
    "\n",
    "# Calculate max length with stop token\n",
    "max_len = max(max(len(q) for q in questions), max(len(a) for a in answers))\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [pad_token] * (max_len - len(seq))\n",
    "\n",
    "questions = [pad_sequence(seq, max_len) for seq in questions]\n",
    "answers = [pad_sequence(seq, max_len) for seq in answers]\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = DialogueDataset(questions, answers)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# LSTM Model with stop token handling\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.stop_token = stop_token\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "model = LSTMModel(len(vocab.word2idx))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    batch_progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for batch_X, batch_y in batch_progress:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs.view(-1, len(vocab.word2idx)), batch_y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "# Enhanced response generation with stop token\n",
    "def generate_response(input_text):\n",
    "    model.eval()\n",
    "    sequence = text_to_sequence(input_text, vocab)\n",
    "    padded = pad_sequence(sequence, max_len)\n",
    "    input_tensor = torch.LongTensor(padded).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "    \n",
    "    response = []\n",
    "    stop_token = vocab.word2idx[STOP_TOKEN]\n",
    "    for step in range(max_len):\n",
    "        probs = torch.softmax(outputs[0, step], dim=0)\n",
    "        predicted = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        if predicted == stop_token:\n",
    "            break\n",
    "        if predicted != pad_token and predicted != stop_token:\n",
    "            response.append(vocab.idx2word.get(predicted, UNK_TOKEN))\n",
    "    \n",
    "    return ' '.join(response)\n",
    "\n",
    "# Test the model\n",
    "test_input = \"hi, how are you doing?\"\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Response: {generate_response(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a67c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hi, how are you doing?\n",
      "Response: how two the to to funny-looking.\n"
     ]
    }
   ],
   "source": [
    "test_input = \"hi, how are you doing?\"\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Response: {generate_response(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: bert-score in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: nltk in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (3.9.1)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from rouge-score) (2.2.5)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (4.51.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (3.10.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: click in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (80.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from requests->bert-score) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rk225\\documents\\project_college\\college-chatbot-v3\\venv\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=e8d5f50efd25c233327b8a64185d35f8d54f9a8dc6ac6f2390ca2f74800225bd\n",
      "  Stored in directory: c:\\users\\rk225\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.2.2 rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rk225\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge-score bert-score nltk\n",
    "# !python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de7890a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2e3886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and strip whitespace.\"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def exact_match(pred, true):\n",
    "    \"\"\"Compute Exact Match (case and punctuation insensitive).\"\"\"\n",
    "    return int(normalize_text(pred) == normalize_text(true))\n",
    "\n",
    "def compute_token_f1(pred, true):\n",
    "    \"\"\"Compute token-level F1 score with word counts (bag-of-words).\"\"\"\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    true_tokens = normalize_text(true).split()\n",
    "\n",
    "    pred_counter = Counter(pred_tokens)\n",
    "    true_counter = Counter(true_tokens)\n",
    "    common_tokens = pred_counter & true_counter\n",
    "    num_common = sum(common_tokens.values())\n",
    "\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_common / len(pred_tokens) if pred_tokens else 0.0\n",
    "    recall = num_common / len(true_tokens) if true_tokens else 0.0\n",
    "\n",
    "    if (precision + recall) == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def compute_rouge_l(pred, true):\n",
    "    \"\"\"Compute ROUGE-L score.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(true, pred)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def compute_meteor(pred, true):\n",
    "    \"\"\"Compute METEOR score with tokenization.\"\"\"\n",
    "    pred_tokens = word_tokenize(pred.lower())\n",
    "    true_tokens = word_tokenize(true.lower())\n",
    "    return meteor_score([true_tokens], pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b870d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "\n",
    "class ERMBART(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, k=10):\n",
    "        super().__init__(config)\n",
    "        self.d = config.d_model  # Hidden size (768 for base, 1024 for large)\n",
    "        self.k = k  # Number of memory slots\n",
    "        \n",
    "        # Entailment Relation Memory (ERM)\n",
    "        self.memory = nn.Parameter(torch.randn(k, self.d))  # [k, d]\n",
    "        self.W_pi = nn.Linear(self.d, k)  # Memory attention weights\n",
    "        \n",
    "        # Initialize memory and projection layer\n",
    "        nn.init.xavier_normal_(self.memory)\n",
    "        nn.init.xavier_normal_(self.W_pi.weight)\n",
    "\n",
    "    def compute_z(self, inputs_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute latent memory vector using mean pooling\"\"\"\n",
    "        # inputs_embeds: [batch_size, seq_len, d]\n",
    "        pooled = inputs_embeds.mean(dim=1)  # [batch_size, d]\n",
    "        pi = torch.softmax(self.W_pi(pooled), dim=-1)  # [batch_size, k]\n",
    "        z = torch.matmul(pi, self.memory)  # [batch_size, d]\n",
    "        return z\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: torch.LongTensor = None,\n",
    "        decoder_input_ids: torch.LongTensor = None,\n",
    "        decoder_attention_mask: torch.LongTensor = None,\n",
    "        labels: torch.LongTensor = None,\n",
    "        **kwargs\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "        # Encode inputs\n",
    "        inputs_embeds = self.model.encoder.embed_tokens(input_ids)\n",
    "        z = self.compute_z(inputs_embeds)  # [batch_size, d]\n",
    "\n",
    "        # Handle decoder inputs for training\n",
    "        if labels is not None:\n",
    "            # Shift labels for autoregressive training\n",
    "            decoder_input_ids = shift_tokens_right(\n",
    "                labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Get decoder embeddings\n",
    "        decoder_inputs_embeds = self.model.decoder.embed_tokens(decoder_input_ids)\n",
    "        \n",
    "        # Inject latent memory into first decoder token\n",
    "        decoder_inputs_embeds[:, 0] += z  # [batch_size, d]\n",
    "\n",
    "        # Forward through original BART architecture\n",
    "        return super().forward(\n",
    "            input_ids=None,  # We're using embeddings directly\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def generate(self, input_ids: torch.LongTensor, **kwargs):\n",
    "        \"\"\"Fixed generation method with proper input handling\"\"\"\n",
    "        # Compute encoder embeddings from input_ids\n",
    "        inputs_embeds = self.model.encoder.embed_tokens(input_ids)\n",
    "        z = self.compute_z(inputs_embeds)\n",
    "        \n",
    "        # Create initial decoder input with start token\n",
    "        decoder_start = torch.full(\n",
    "            (input_ids.size(0), 1),\n",
    "            self.config.decoder_start_token_id,\n",
    "            device=input_ids.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Get decoder embeddings and inject memory\n",
    "        decoder_inputs_embeds = self.model.decoder.embed_tokens(decoder_start)\n",
    "        decoder_inputs_embeds[:, 0] += z\n",
    "        \n",
    "        # Generate using both encoder and decoder embeddings\n",
    "        return super().generate(\n",
    "            input_ids=input_ids,  # Maintain original input_ids\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca91a637",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Tokenize input with proper padding\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms your favorite food?\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      4\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     max_length=\u001b[32m128\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m     11\u001b[39m output = model.generate(\n\u001b[32m     12\u001b[39m     input_ids=inputs.input_ids.to(device),\n\u001b[32m     13\u001b[39m     attention_mask=inputs.attention_mask.to(device),\n\u001b[32m     14\u001b[39m     max_length=\u001b[32m50\u001b[39m,\n\u001b[32m     15\u001b[39m     num_beams=\u001b[32m5\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize input with proper padding\n",
    "inputs = tokenizer(\n",
    "    \"What's your favorite food?\", \n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    input_ids=inputs.input_ids.to(device),\n",
    "    attention_mask=inputs.attention_mask.to(device),\n",
    "    max_length=50,\n",
    "    num_beams=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
