{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "class EntailmentMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, hidden_dim))  # [K, D]\n",
    "        self.proj = nn.Linear(hidden_dim, num_slots)  # Maps BART hidden states to memory slots\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_state: [batch_size, hidden_dim] (e.g., BART's [z] token embedding)\n",
    "        Returns:\n",
    "            z: Entailment representation [batch_size, hidden_dim]\n",
    "            attn_weights: Memory attention scores [batch_size, num_slots]\n",
    "        \"\"\"\n",
    "        attn_weights = torch.softmax(self.proj(hidden_state), dim=-1)  # [batch_size, K]\n",
    "        z = torch.einsum('bk,kd->bd', attn_weights, self.memory)      # [batch_size, D]\n",
    "        return z, attn_weights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiscourseMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, hidden_dim))  # [L, D]\n",
    "        self.proj = nn.Linear(hidden_dim, num_slots)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Same as ERM but for discourse features.\"\"\"\n",
    "        attn_weights = torch.softmax(self.proj(hidden_state), dim=-1)  # [batch_size, L]\n",
    "        z_d = torch.einsum('bl,ld->bd', attn_weights, self.memory)     # [batch_size, D]\n",
    "        return z_d, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e494144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Dict, Any\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BartForConditionalGeneration, BartConfig\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqLMOutput\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBARTWithMemory\u001b[39;00m(nn.Module):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1953\u001b[39m     value = Placeholder\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1965\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1969\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1970\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1972\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\models\\__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     albert,\n\u001b[32m     17\u001b[39m     align,\n\u001b[32m     18\u001b[39m     altclip,\n\u001b[32m     19\u001b[39m     aria,\n\u001b[32m     20\u001b[39m     audio_spectrogram_transformer,\n\u001b[32m     21\u001b[39m     auto,\n\u001b[32m     22\u001b[39m     autoformer,\n\u001b[32m     23\u001b[39m     aya_vision,\n\u001b[32m     24\u001b[39m     bamba,\n\u001b[32m     25\u001b[39m     bark,\n\u001b[32m     26\u001b[39m     bart,\n\u001b[32m     27\u001b[39m     barthez,\n\u001b[32m     28\u001b[39m     bartpho,\n\u001b[32m     29\u001b[39m     beit,\n\u001b[32m     30\u001b[39m     bert,\n\u001b[32m     31\u001b[39m     bert_generation,\n\u001b[32m     32\u001b[39m     bert_japanese,\n\u001b[32m     33\u001b[39m     bertweet,\n\u001b[32m     34\u001b[39m     big_bird,\n\u001b[32m     35\u001b[39m     bigbird_pegasus,\n\u001b[32m     36\u001b[39m     biogpt,\n\u001b[32m     37\u001b[39m     bit,\n\u001b[32m     38\u001b[39m     blenderbot,\n\u001b[32m     39\u001b[39m     blenderbot_small,\n\u001b[32m     40\u001b[39m     blip,\n\u001b[32m     41\u001b[39m     blip_2,\n\u001b[32m     42\u001b[39m     bloom,\n\u001b[32m     43\u001b[39m     bridgetower,\n\u001b[32m     44\u001b[39m     bros,\n\u001b[32m     45\u001b[39m     byt5,\n\u001b[32m     46\u001b[39m     camembert,\n\u001b[32m     47\u001b[39m     canine,\n\u001b[32m     48\u001b[39m     chameleon,\n\u001b[32m     49\u001b[39m     chinese_clip,\n\u001b[32m     50\u001b[39m     clap,\n\u001b[32m     51\u001b[39m     clip,\n\u001b[32m     52\u001b[39m     clipseg,\n\u001b[32m     53\u001b[39m     clvp,\n\u001b[32m     54\u001b[39m     code_llama,\n\u001b[32m     55\u001b[39m     codegen,\n\u001b[32m     56\u001b[39m     cohere,\n\u001b[32m     57\u001b[39m     cohere2,\n\u001b[32m     58\u001b[39m     colpali,\n\u001b[32m     59\u001b[39m     conditional_detr,\n\u001b[32m     60\u001b[39m     convbert,\n\u001b[32m     61\u001b[39m     convnext,\n\u001b[32m     62\u001b[39m     convnextv2,\n\u001b[32m     63\u001b[39m     cpm,\n\u001b[32m     64\u001b[39m     cpmant,\n\u001b[32m     65\u001b[39m     ctrl,\n\u001b[32m     66\u001b[39m     cvt,\n\u001b[32m     67\u001b[39m     dab_detr,\n\u001b[32m     68\u001b[39m     dac,\n\u001b[32m     69\u001b[39m     data2vec,\n\u001b[32m     70\u001b[39m     dbrx,\n\u001b[32m     71\u001b[39m     deberta,\n\u001b[32m     72\u001b[39m     deberta_v2,\n\u001b[32m     73\u001b[39m     decision_transformer,\n\u001b[32m     74\u001b[39m     deepseek_v3,\n\u001b[32m     75\u001b[39m     deformable_detr,\n\u001b[32m     76\u001b[39m     deit,\n\u001b[32m     77\u001b[39m     deprecated,\n\u001b[32m     78\u001b[39m     depth_anything,\n\u001b[32m     79\u001b[39m     depth_pro,\n\u001b[32m     80\u001b[39m     detr,\n\u001b[32m     81\u001b[39m     dialogpt,\n\u001b[32m     82\u001b[39m     diffllama,\n\u001b[32m     83\u001b[39m     dinat,\n\u001b[32m     84\u001b[39m     dinov2,\n\u001b[32m     85\u001b[39m     dinov2_with_registers,\n\u001b[32m     86\u001b[39m     distilbert,\n\u001b[32m     87\u001b[39m     dit,\n\u001b[32m     88\u001b[39m     donut,\n\u001b[32m     89\u001b[39m     dpr,\n\u001b[32m     90\u001b[39m     dpt,\n\u001b[32m     91\u001b[39m     efficientnet,\n\u001b[32m     92\u001b[39m     electra,\n\u001b[32m     93\u001b[39m     emu3,\n\u001b[32m     94\u001b[39m     encodec,\n\u001b[32m     95\u001b[39m     encoder_decoder,\n\u001b[32m     96\u001b[39m     ernie,\n\u001b[32m     97\u001b[39m     esm,\n\u001b[32m     98\u001b[39m     falcon,\n\u001b[32m     99\u001b[39m     falcon_mamba,\n\u001b[32m    100\u001b[39m     fastspeech2_conformer,\n\u001b[32m    101\u001b[39m     flaubert,\n\u001b[32m    102\u001b[39m     flava,\n\u001b[32m    103\u001b[39m     fnet,\n\u001b[32m    104\u001b[39m     focalnet,\n\u001b[32m    105\u001b[39m     fsmt,\n\u001b[32m    106\u001b[39m     funnel,\n\u001b[32m    107\u001b[39m     fuyu,\n\u001b[32m    108\u001b[39m     gemma,\n\u001b[32m    109\u001b[39m     gemma2,\n\u001b[32m    110\u001b[39m     gemma3,\n\u001b[32m    111\u001b[39m     git,\n\u001b[32m    112\u001b[39m     glm,\n\u001b[32m    113\u001b[39m     glm4,\n\u001b[32m    114\u001b[39m     glpn,\n\u001b[32m    115\u001b[39m     got_ocr2,\n\u001b[32m    116\u001b[39m     gpt2,\n\u001b[32m    117\u001b[39m     gpt_bigcode,\n\u001b[32m    118\u001b[39m     gpt_neo,\n\u001b[32m    119\u001b[39m     gpt_neox,\n\u001b[32m    120\u001b[39m     gpt_neox_japanese,\n\u001b[32m    121\u001b[39m     gpt_sw3,\n\u001b[32m    122\u001b[39m     gptj,\n\u001b[32m    123\u001b[39m     granite,\n\u001b[32m    124\u001b[39m     granitemoe,\n\u001b[32m    125\u001b[39m     granitemoeshared,\n\u001b[32m    126\u001b[39m     grounding_dino,\n\u001b[32m    127\u001b[39m     groupvit,\n\u001b[32m    128\u001b[39m     helium,\n\u001b[32m    129\u001b[39m     herbert,\n\u001b[32m    130\u001b[39m     hiera,\n\u001b[32m    131\u001b[39m     hubert,\n\u001b[32m    132\u001b[39m     ibert,\n\u001b[32m    133\u001b[39m     idefics,\n\u001b[32m    134\u001b[39m     idefics2,\n\u001b[32m    135\u001b[39m     idefics3,\n\u001b[32m    136\u001b[39m     ijepa,\n\u001b[32m    137\u001b[39m     imagegpt,\n\u001b[32m    138\u001b[39m     informer,\n\u001b[32m    139\u001b[39m     instructblip,\n\u001b[32m    140\u001b[39m     instructblipvideo,\n\u001b[32m    141\u001b[39m     jamba,\n\u001b[32m    142\u001b[39m     jetmoe,\n\u001b[32m    143\u001b[39m     kosmos2,\n\u001b[32m    144\u001b[39m     layoutlm,\n\u001b[32m    145\u001b[39m     layoutlmv2,\n\u001b[32m    146\u001b[39m     layoutlmv3,\n\u001b[32m    147\u001b[39m     layoutxlm,\n\u001b[32m    148\u001b[39m     led,\n\u001b[32m    149\u001b[39m     levit,\n\u001b[32m    150\u001b[39m     lilt,\n\u001b[32m    151\u001b[39m     llama,\n\u001b[32m    152\u001b[39m     llama4,\n\u001b[32m    153\u001b[39m     llava,\n\u001b[32m    154\u001b[39m     llava_next,\n\u001b[32m    155\u001b[39m     llava_next_video,\n\u001b[32m    156\u001b[39m     llava_onevision,\n\u001b[32m    157\u001b[39m     longformer,\n\u001b[32m    158\u001b[39m     longt5,\n\u001b[32m    159\u001b[39m     luke,\n\u001b[32m    160\u001b[39m     lxmert,\n\u001b[32m    161\u001b[39m     m2m_100,\n\u001b[32m    162\u001b[39m     mamba,\n\u001b[32m    163\u001b[39m     mamba2,\n\u001b[32m    164\u001b[39m     marian,\n\u001b[32m    165\u001b[39m     markuplm,\n\u001b[32m    166\u001b[39m     mask2former,\n\u001b[32m    167\u001b[39m     maskformer,\n\u001b[32m    168\u001b[39m     mbart,\n\u001b[32m    169\u001b[39m     mbart50,\n\u001b[32m    170\u001b[39m     megatron_bert,\n\u001b[32m    171\u001b[39m     megatron_gpt2,\n\u001b[32m    172\u001b[39m     mgp_str,\n\u001b[32m    173\u001b[39m     mimi,\n\u001b[32m    174\u001b[39m     mistral,\n\u001b[32m    175\u001b[39m     mistral3,\n\u001b[32m    176\u001b[39m     mixtral,\n\u001b[32m    177\u001b[39m     mllama,\n\u001b[32m    178\u001b[39m     mluke,\n\u001b[32m    179\u001b[39m     mobilebert,\n\u001b[32m    180\u001b[39m     mobilenet_v1,\n\u001b[32m    181\u001b[39m     mobilenet_v2,\n\u001b[32m    182\u001b[39m     mobilevit,\n\u001b[32m    183\u001b[39m     mobilevitv2,\n\u001b[32m    184\u001b[39m     modernbert,\n\u001b[32m    185\u001b[39m     moonshine,\n\u001b[32m    186\u001b[39m     moshi,\n\u001b[32m    187\u001b[39m     mpnet,\n\u001b[32m    188\u001b[39m     mpt,\n\u001b[32m    189\u001b[39m     mra,\n\u001b[32m    190\u001b[39m     mt5,\n\u001b[32m    191\u001b[39m     musicgen,\n\u001b[32m    192\u001b[39m     musicgen_melody,\n\u001b[32m    193\u001b[39m     mvp,\n\u001b[32m    194\u001b[39m     myt5,\n\u001b[32m    195\u001b[39m     nemotron,\n\u001b[32m    196\u001b[39m     nllb,\n\u001b[32m    197\u001b[39m     nllb_moe,\n\u001b[32m    198\u001b[39m     nougat,\n\u001b[32m    199\u001b[39m     nystromformer,\n\u001b[32m    200\u001b[39m     olmo,\n\u001b[32m    201\u001b[39m     olmo2,\n\u001b[32m    202\u001b[39m     olmoe,\n\u001b[32m    203\u001b[39m     omdet_turbo,\n\u001b[32m    204\u001b[39m     oneformer,\n\u001b[32m    205\u001b[39m     openai,\n\u001b[32m    206\u001b[39m     opt,\n\u001b[32m    207\u001b[39m     owlv2,\n\u001b[32m    208\u001b[39m     owlvit,\n\u001b[32m    209\u001b[39m     paligemma,\n\u001b[32m    210\u001b[39m     patchtsmixer,\n\u001b[32m    211\u001b[39m     patchtst,\n\u001b[32m    212\u001b[39m     pegasus,\n\u001b[32m    213\u001b[39m     pegasus_x,\n\u001b[32m    214\u001b[39m     perceiver,\n\u001b[32m    215\u001b[39m     persimmon,\n\u001b[32m    216\u001b[39m     phi,\n\u001b[32m    217\u001b[39m     phi3,\n\u001b[32m    218\u001b[39m     phi4_multimodal,\n\u001b[32m    219\u001b[39m     phimoe,\n\u001b[32m    220\u001b[39m     phobert,\n\u001b[32m    221\u001b[39m     pix2struct,\n\u001b[32m    222\u001b[39m     pixtral,\n\u001b[32m    223\u001b[39m     plbart,\n\u001b[32m    224\u001b[39m     poolformer,\n\u001b[32m    225\u001b[39m     pop2piano,\n\u001b[32m    226\u001b[39m     prompt_depth_anything,\n\u001b[32m    227\u001b[39m     prophetnet,\n\u001b[32m    228\u001b[39m     pvt,\n\u001b[32m    229\u001b[39m     pvt_v2,\n\u001b[32m    230\u001b[39m     qwen2,\n\u001b[32m    231\u001b[39m     qwen2_5_vl,\n\u001b[32m    232\u001b[39m     qwen2_audio,\n\u001b[32m    233\u001b[39m     qwen2_moe,\n\u001b[32m    234\u001b[39m     qwen2_vl,\n\u001b[32m    235\u001b[39m     qwen3,\n\u001b[32m    236\u001b[39m     qwen3_moe,\n\u001b[32m    237\u001b[39m     rag,\n\u001b[32m    238\u001b[39m     recurrent_gemma,\n\u001b[32m    239\u001b[39m     reformer,\n\u001b[32m    240\u001b[39m     regnet,\n\u001b[32m    241\u001b[39m     rembert,\n\u001b[32m    242\u001b[39m     resnet,\n\u001b[32m    243\u001b[39m     roberta,\n\u001b[32m    244\u001b[39m     roberta_prelayernorm,\n\u001b[32m    245\u001b[39m     roc_bert,\n\u001b[32m    246\u001b[39m     roformer,\n\u001b[32m    247\u001b[39m     rt_detr,\n\u001b[32m    248\u001b[39m     rt_detr_v2,\n\u001b[32m    249\u001b[39m     rwkv,\n\u001b[32m    250\u001b[39m     sam,\n\u001b[32m    251\u001b[39m     seamless_m4t,\n\u001b[32m    252\u001b[39m     seamless_m4t_v2,\n\u001b[32m    253\u001b[39m     segformer,\n\u001b[32m    254\u001b[39m     seggpt,\n\u001b[32m    255\u001b[39m     sew,\n\u001b[32m    256\u001b[39m     sew_d,\n\u001b[32m    257\u001b[39m     shieldgemma2,\n\u001b[32m    258\u001b[39m     siglip,\n\u001b[32m    259\u001b[39m     siglip2,\n\u001b[32m    260\u001b[39m     smolvlm,\n\u001b[32m    261\u001b[39m     speech_encoder_decoder,\n\u001b[32m    262\u001b[39m     speech_to_text,\n\u001b[32m    263\u001b[39m     speecht5,\n\u001b[32m    264\u001b[39m     splinter,\n\u001b[32m    265\u001b[39m     squeezebert,\n\u001b[32m    266\u001b[39m     stablelm,\n\u001b[32m    267\u001b[39m     starcoder2,\n\u001b[32m    268\u001b[39m     superglue,\n\u001b[32m    269\u001b[39m     superpoint,\n\u001b[32m    270\u001b[39m     swiftformer,\n\u001b[32m    271\u001b[39m     swin,\n\u001b[32m    272\u001b[39m     swin2sr,\n\u001b[32m    273\u001b[39m     swinv2,\n\u001b[32m    274\u001b[39m     switch_transformers,\n\u001b[32m    275\u001b[39m     t5,\n\u001b[32m    276\u001b[39m     table_transformer,\n\u001b[32m    277\u001b[39m     tapas,\n\u001b[32m    278\u001b[39m     textnet,\n\u001b[32m    279\u001b[39m     time_series_transformer,\n\u001b[32m    280\u001b[39m     timesformer,\n\u001b[32m    281\u001b[39m     timm_backbone,\n\u001b[32m    282\u001b[39m     timm_wrapper,\n\u001b[32m    283\u001b[39m     trocr,\n\u001b[32m    284\u001b[39m     tvp,\n\u001b[32m    285\u001b[39m     udop,\n\u001b[32m    286\u001b[39m     umt5,\n\u001b[32m    287\u001b[39m     unispeech,\n\u001b[32m    288\u001b[39m     unispeech_sat,\n\u001b[32m    289\u001b[39m     univnet,\n\u001b[32m    290\u001b[39m     upernet,\n\u001b[32m    291\u001b[39m     video_llava,\n\u001b[32m    292\u001b[39m     videomae,\n\u001b[32m    293\u001b[39m     vilt,\n\u001b[32m    294\u001b[39m     vipllava,\n\u001b[32m    295\u001b[39m     vision_encoder_decoder,\n\u001b[32m    296\u001b[39m     vision_text_dual_encoder,\n\u001b[32m    297\u001b[39m     visual_bert,\n\u001b[32m    298\u001b[39m     vit,\n\u001b[32m    299\u001b[39m     vit_mae,\n\u001b[32m    300\u001b[39m     vit_msn,\n\u001b[32m    301\u001b[39m     vitdet,\n\u001b[32m    302\u001b[39m     vitmatte,\n\u001b[32m    303\u001b[39m     vitpose,\n\u001b[32m    304\u001b[39m     vitpose_backbone,\n\u001b[32m    305\u001b[39m     vits,\n\u001b[32m    306\u001b[39m     vivit,\n\u001b[32m    307\u001b[39m     wav2vec2,\n\u001b[32m    308\u001b[39m     wav2vec2_bert,\n\u001b[32m    309\u001b[39m     wav2vec2_conformer,\n\u001b[32m    310\u001b[39m     wav2vec2_phoneme,\n\u001b[32m    311\u001b[39m     wav2vec2_with_lm,\n\u001b[32m    312\u001b[39m     wavlm,\n\u001b[32m    313\u001b[39m     whisper,\n\u001b[32m    314\u001b[39m     x_clip,\n\u001b[32m    315\u001b[39m     xglm,\n\u001b[32m    316\u001b[39m     xlm,\n\u001b[32m    317\u001b[39m     xlm_roberta,\n\u001b[32m    318\u001b[39m     xlm_roberta_xl,\n\u001b[32m    319\u001b[39m     xlnet,\n\u001b[32m    320\u001b[39m     xmod,\n\u001b[32m    321\u001b[39m     yolos,\n\u001b[32m    322\u001b[39m     yoso,\n\u001b[32m    323\u001b[39m     zamba,\n\u001b[32m    324\u001b[39m     zamba2,\n\u001b[32m    325\u001b[39m     zoedepth,\n\u001b[32m    326\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\__init__.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     29\u001b[39m _file = \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\u001b[34m__name__\u001b[39m, _file, \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m, module_spec=__spec__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2391\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2371\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2372\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2373\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2389\u001b[39m \u001b[33;03m    The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.\u001b[39;00m\n\u001b[32m   2390\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2391\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m spread_import_structure(import_structure)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2161\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2159\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2161\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   2162\u001b[39m     file_content = f.read()\n\u001b[32m   2164\u001b[39m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BARTWithMemory(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bart_model_name: str = \"facebook/bart-base\",\n",
    "                 erm_slots: int = 10, \n",
    "                 ddm_slots: int = 5):\n",
    "        super().__init__()\n",
    "        # Load full model to preserve components\n",
    "        full_bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "        self.bart = full_bart.model\n",
    "        self.lm_head = full_bart.lm_head\n",
    "        self.final_logits_bias = full_bart.final_logits_bias\n",
    "        self.config: BartConfig = full_bart.config\n",
    "        # self.generate = full_bart.generate\n",
    "\n",
    "        # Initialize memory modules\n",
    "        self.erm = EntailmentMemory(erm_slots, self.config.d_model)\n",
    "        self.ddm = DiscourseMemory(ddm_slots, self.config.d_model)\n",
    "        self.ortho_loss_coeff = 0.1\n",
    "\n",
    "        # Register token IDs from config\n",
    "        self.register_buffer(\"sop_token_id\", torch.tensor([self.config.bos_token_id]))\n",
    "        self.register_buffer(\"eop_token_id\", torch.tensor([self.config.eos_token_id]))\n",
    "\n",
    "    def orthogonal_loss(self) -> torch.Tensor:\n",
    "        \"\"\"Orthogonality constraint between memory matrices\"\"\"\n",
    "        return torch.norm(torch.mm(self.erm.memory, self.ddm.memory.T)) ** 2\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Encode inputs\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.bart.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        \n",
    "        last_hidden = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Memory operations\n",
    "        z_token_embedding = last_hidden[:, 0]  # First token embedding\n",
    "        z, _ = self.erm(z_token_embedding)\n",
    "        z_d, _ = self.ddm(z_token_embedding)\n",
    "\n",
    "        # Prepare decoder inputs\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = self._shift_right(labels)\n",
    "        \n",
    "        # Embed decoder inputs\n",
    "        decoder_inputs_embeds = self.bart.decoder.embed_tokens(decoder_input_ids)\n",
    "        decoder_inputs_embeds[:, 0] += z + z_d  # Modify first token\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.bart.decoder(\n",
    "            input_ids=None,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            encoder_hidden_states=last_hidden,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Calculate logits and losses\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state) + self.final_logits_bias\n",
    "        loss = None\n",
    "        ortho_loss = self.orthogonal_loss()\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n",
    "            ce_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            loss = ce_loss + self.ortho_loss_coeff * ortho_loss\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _shift_right(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Shift labels to create decoder inputs (like original BART)\"\"\"\n",
    "        shifted = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted[:, 0] = self.sop_token_id\n",
    "        return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5d6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BARTWithMemory' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m input_ids = tokenizer(\u001b[33m\"\u001b[39m\u001b[33mHello, how are you?\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_ids\n\u001b[32m      5\u001b[39m labels = tokenizer(\u001b[33m\"\u001b[39m\u001b[33mI am fine, thank you.\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_ids\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m(input_ids=input_ids, labels=labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'BARTWithMemory' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BARTWithMemory()\n",
    "input_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"I am fine, thank you.\", return_tensors=\"pt\").input_ids\n",
    "model.generate(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5500db62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BartTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenizer = \u001b[43mBartTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/bart-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model = BARTWithMemory()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Structured example\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2062\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2059\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2175\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2171\u001b[39m     config = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2173\u001b[39m     \u001b[38;5;66;03m# Third attempt. If we have not yet found the original type of the tokenizer,\u001b[39;00m\n\u001b[32m   2174\u001b[39m     \u001b[38;5;66;03m# we are loading we see if we can infer it from the type of the configuration file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2175\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_MAPPING_NAMES  \u001b[38;5;66;03m# tests_ignore\u001b[39;00m\n\u001b[32m   2177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2178\u001b[39m         model_type = config.model_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:38\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     cached_file,\n\u001b[32m     31\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     logging,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     41\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:40\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     43\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m CLASS_DOCSTRING = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m    This is a generic model class that will be instantiated as one of the model classes of the library when created\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[33m    with the [`~BaseAutoModelClass.from_pretrained`] class method or the [`~BaseAutoModelClass.from_config`] class\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33m    This class cannot be instantiated directly using `__init__()` (throws an error).\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1953\u001b[39m     value = Placeholder\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1965\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1969\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1970\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1972\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcandidate_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantVocabTranslatorCache\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     Cache,\n\u001b[32m     34\u001b[39m     DynamicCache,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     StaticCache,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py:27\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_sklearn_available\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\sklearn\\utils\\_joblib.py:12\u001b[39m\n\u001b[32m      9\u001b[39m     _warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# versions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m         Memory,\n\u001b[32m     15\u001b[39m         Parallel,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         register_parallel_backend,\n\u001b[32m     26\u001b[39m     )\n\u001b[32m     29\u001b[39m __all__ = [\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparallel_backend\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_parallel_backend\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\joblib\\__init__.py:114\u001b[39m\n\u001b[32m    109\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.5.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cloudpickle_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_non_picklable_objects\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_parallel_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelBackendBase\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_store_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StoreBackendBase\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\joblib\\_cloudpickle_wrapper.py:14\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloky\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_non_picklable_objects\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     wrap_non_picklable_objects = _my_wrap_non_picklable_objects\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\__init__.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreduction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_loky_pickler\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreusable_executor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_reusable_executor\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloudpickle_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_non_picklable_objects\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocess_executor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrokenProcessPool, ProcessPoolExecutor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocess_executor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessPoolExecutor, EXTRA_QUEUED_CALLS\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqueues\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Queue, SimpleQueue\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreduction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_loky_pickler, get_loky_pickler_name\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kill_process_tree, get_exitcodes_terminated_worker\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minitializers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prepare_initializer\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Mechanism to prevent infinite process spawning. When a worker of a\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# ProcessPoolExecutor nested in MAX_DEPTH Executor tries to create a new\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Executor, a LokyRecursionError is raised\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1528\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1502\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1601\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BARTWithMemory()\n",
    "\n",
    "# Structured example\n",
    "input_text = \"<latent><persona>p1,p2<query>q<response>\"\n",
    "output_text = \"r<eos>\"\n",
    "\n",
    "# Tokenize (make sure to add special tokens to your tokenizer first!)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "labels = tokenizer(output_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e153da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3052.6052, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4715f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m decoder_input_ids = tokenizer(\u001b[33m\"\u001b[39m\u001b[33m<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_ids\n\u001b[32m      5\u001b[39m output_ids = model.generate(\n\u001b[32m      6\u001b[39m     input_ids=inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m     attention_mask=inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     pad_token_id=tokenizer.eos_token_id\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "\n",
    "inputs = tokenizer(\"<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \", return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenizer(\"<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \", return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and extract response\n",
    "full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# response = full_output.split(\"<response>\")[-1].split(\"<eos>\")[0].strip()\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbac0c",
   "metadata": {},
   "source": [
    "### NExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d0b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration, BartConfig, BartModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class EntailmentMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, embedding_dim))\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # x: [batch_size, embedding_dim]\n",
    "        projected = self.proj(x)  # [batch_size, embedding_dim]\n",
    "        scores = torch.matmul(projected, self.memory.T)  # [batch_size, num_slots]\n",
    "        weights = self.softmax(scores)\n",
    "        output = torch.matmul(weights, self.memory)  # [batch_size, embedding_dim]\n",
    "        return output, weights\n",
    "\n",
    "class DiscourseMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, embedding_dim))\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        projected = self.proj(x)\n",
    "        scores = torch.matmul(projected, self.memory.T)\n",
    "        weights = self.softmax(scores)\n",
    "        output = torch.matmul(weights, self.memory)\n",
    "        return output, weights\n",
    "\n",
    "class BartModelWithMemory(BartModel):\n",
    "    def __init__(self, config: BartConfig, erm_slots: int = 10, ddm_slots: int = 5):\n",
    "        super().__init__(config)\n",
    "        self.erm = EntailmentMemory(erm_slots, config.d_model)\n",
    "        self.ddm = DiscourseMemory(ddm_slots, config.d_model)\n",
    "        self.ortho_loss_coeff = 0.1\n",
    "\n",
    "    def orthogonal_loss(self) -> torch.Tensor:\n",
    "        return torch.norm(torch.mm(self.erm.memory, self.ddm.memory.T)) ** 2\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        # Original BART forward\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Memory operations only on initial forward pass\n",
    "        if past_key_values is None:\n",
    "            encoder_hidden = outputs.encoder_last_hidden_state\n",
    "            z_token = encoder_hidden[:, 0]  # Use first token embedding\n",
    "            \n",
    "            # Get memory outputs\n",
    "            erm_out, _ = self.erm(z_token)\n",
    "            ddm_out, _ = self.ddm(z_token)\n",
    "            \n",
    "            # Modify decoder inputs\n",
    "            if decoder_inputs_embeds is None:\n",
    "                decoder_inputs_embeds = self.decoder.embed_tokens(decoder_input_ids)\n",
    "            \n",
    "            # Apply memory to decoder's first token embedding\n",
    "            decoder_inputs_embeds[:, 0] += erm_out + ddm_out\n",
    "            \n",
    "            # Re-run decoder with modified inputs\n",
    "            outputs = super().forward(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class BARTWithMemory(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, erm_slots: int = 10, ddm_slots: int = 5):\n",
    "        super().__init__(config)\n",
    "        self.model = BartModelWithMemory(config, erm_slots, ddm_slots)\n",
    "        self.post_init()  # Important for loading pretrained weights\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outputs = super().forward(**kwargs)\n",
    "        \n",
    "        # Add orthogonal loss\n",
    "        if kwargs.get('labels') is not None:\n",
    "            ortho_loss = self.model.orthogonal_loss()\n",
    "            outputs.loss += self.model.ortho_loss_coeff * ortho_loss\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc99ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BARTWithMemory were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['model.ddm.memory', 'model.ddm.proj.bias', 'model.ddm.proj.weight', 'model.erm.memory', 'model.erm.proj.bias', 'model.erm.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import BartTokenizer\n",
    "\n",
    "    # Initialize model\n",
    "    config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "    model = BARTWithMemory.from_pretrained(\n",
    "        \"facebook/bart-base\",\n",
    "        config=config,\n",
    "        erm_slots=10,\n",
    "        ddm_slots=5\n",
    "    )\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    # Sample generation\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate with memory\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=100,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4dd29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "class BARTWithMemory(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, k=10):\n",
    "        super().__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed310ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BARTWithMemory.from_pretrained(\"facebook/bart-base\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb4e541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    0, 1121,   97, 1617,    6,    5,  276,  631, 2594,   77,   47,\n",
       "          214,   45,  447,    4,   96,   42,  403,    6,    2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a32372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoder(\n",
       "  (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "  (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x BartEncoderLayer(\n",
       "      (self_attn): BartSdpaAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation_fn): GELUActivation()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
